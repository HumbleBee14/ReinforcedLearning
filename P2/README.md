# P2: CartPole with DQN (Deep Q-Network)

> **Goal:** Move from Q-tables to neural networks as function approximators.

## What We'll Learn

- Why Q-tables don't scale (state space explosion)
- Neural networks as universal function approximators
- Experience Replay (learning from a "diary")
- Target Networks (stability trick)
- The DQN architecture

```
P2: CartPole with DQN
        â””â”€â”€ Learn: Neural networks as function approximators
        â””â”€â”€ Build: Your first deep RL agent
```

## Key Bridge Concept

```
Q-Table:     Q[state][action] = value     (lookup table)
     â†“
DQN:         Q(state) = NeuralNet(state)  (function approximation)
```

## Prerequisites

- âœ… P0: Frozen Lake (random agent, understanding environments)
- âœ… P1: Cliff Walking (Q-Learning, SARSA)
- âœ… Docs 00-04 (RL Fundamentals, Q-values, exploration)

## Environment

**CartPole-v1** from Gymnasium
- State: [cart_position, cart_velocity, pole_angle, pole_velocity]
- Actions: [push_left, push_right]
- Reward: +1 for each step the pole stays upright
- Goal: Balance pole for 500 steps

## Files (To Be Created)

```
P2/
â”œâ”€â”€ README.md          # This file
â”œâ”€â”€ cartpole_dqn.py    # Main DQN implementation
â”œâ”€â”€ replay_buffer.py   # Experience replay
â”œâ”€â”€ dqn_network.py     # Neural network architecture
â””â”€â”€ train.py           # Training script
```

## Status: ðŸ”œ Coming Next
