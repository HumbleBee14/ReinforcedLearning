# P4: PPO - Proximal Policy Optimization

> **Goal:** Implement PPO - the algorithm behind RLHF (ChatGPT, InstructGPT).

## What We'll Learn

- Why vanilla policy gradient is unstable
- Trust region methods
- PPO clipping mechanism
- KL divergence constraint
- GAE (Generalized Advantage Estimation)

```
PPO from scratch (or Stable Baselines3)
        â””â”€â”€ Learn: Clipping, KL divergence, advantages
        â””â”€â”€ Build: PPO agent on simple environment
```

## Key Bridge Concept

```
Policy Gradient:  Can make too-large updates, unstable
     â†“
PPO:              Clips updates to stay "close" to old policy
     â†“
RLHF:             PPO + Reward Model = ChatGPT training!
```


## Prerequisites

- âœ… P0-P3: All previous projects
- âœ… Docs 00-06

## The PPO Objective

```python
L_CLIP = E[min(r(Î¸) * A, clip(r(Î¸), 1-Îµ, 1+Îµ) * A)]

Where:
- r(Î¸) = Ï€_new / Ï€_old  (probability ratio)
- A = advantage
- Îµ = 0.2 (clip range)
```

## Files (To Be Created)

```
P4/
â”œâ”€â”€ README.md          # This file
â”œâ”€â”€ ppo.py             # PPO algorithm
â”œâ”€â”€ ppo_agent.py       # Agent wrapper
â”œâ”€â”€ gae.py             # Generalized Advantage Estimation
â””â”€â”€ train.py           # Training script
```

## Status: ðŸ”œ Coming After P3
