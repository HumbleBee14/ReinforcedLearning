# P5: RLHF & DPO for LLMs

> **Goal:** Fine-tune a small LLM using RLHF/DPO principles.

## What We'll Learn

- Reward Model training
- RLHF pipeline (SFT â†’ RM â†’ PPO)
- DPO as simpler alternative
- Preference data format
- TRL library usage

## Key Concept

```
RLHF (3 steps):  SFT â†’ Train Reward Model â†’ PPO
     â†“
DPO (1 step):    Directly optimize on preferences
                 (Mathematically equivalent, simpler!)
```
```
 RLHF/DPO for LLMs
        â””â”€â”€ Learn: TRL library, preference data format
        â””â”€â”€ Build: Fine-tune small model with DPO
        â””â”€â”€ Build: Your Roopik AI-as-judge system!
```

## Prerequisites

- âœ… P0-P4: All RL foundations
- âœ… Understanding of PPO

## Files (To Be Created)

```
P5/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ reward_model.py        # Simple reward model
â”œâ”€â”€ dpo_training.py        # DPO fine-tuning
â”œâ”€â”€ preference_data.json   # Sample preference pairs
â””â”€â”€ evaluate.py            # Evaluate fine-tuned model
```

## Status: ðŸ”œ The Goal!
