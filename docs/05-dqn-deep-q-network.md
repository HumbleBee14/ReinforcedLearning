# DQN: From Tables to Neural Networks

> **Deep Q-Networks** - The algorithm that launched the modern AI revolution.
> DeepMind used DQN to beat Atari games in 2013, proving neural networks could master complex tasks.

NOTE: There's another explanation for the same topic, see **[DQN_EXPLAINED.md](../P2/DQN_EXPLAINED.md)**.
---

## The Problem with Tables

```
Frozen Lake: 16 states Ã— 4 actions = 64 values    âœ… Easy!
Taxi:        500 states Ã— 6 actions = 3,000 values âœ… Manageable
CartPole:    Infinite states (continuous) Ã— 2 actions = ??? âŒ Impossible!
Atari:       210Ã—160Ã—3 pixels = millions of states âŒ Forget it!
```

In tabular Q-learning, we store values in a table. But when states are continuous (like cart position = 0.02345...), we can't create a row for every possible value.

---

## The Solution: Function Approximation

Instead of storing Q(s,a) in a table, we **APPROXIMATE** it with a neural network.

```
Q-Table:    Q[state][action] = lookup_from_table
DQN:        Q(state) = NeuralNet(state) â†’ [Q(a1), Q(a2), ...]
```

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 THE BRIDGE TO LLM RL                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Tabular Q-Learning:  Q[state][action] = value                      â”‚
â”‚       â†“                                                             â”‚
â”‚  DQN:                 Q(state, action) = NeuralNet(state) â†’ value   â”‚
â”‚       â†“                                                             â”‚
â”‚  Policy Gradient:     Ï€(action|state) = NeuralNet(state) â†’ probs    â”‚
â”‚       â†“                                                             â”‚
â”‚  LLM:                 Ï€(token|context) = LLM(context) â†’ probs       â”‚
â”‚                                                                     â”‚
â”‚  It's the same thing, just with bigger neural networks!             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 1: What Goes In and Out?

In DQN, the Neural Network is a **Value Calculator**, not a reflex machine.

### Input: The State

The state describes "the world right now" - a vector of continuous numbers.

```python
# CartPole State (4 numbers):
state = [Cart_Position, Cart_Velocity, Pole_Angle, Pole_Tip_Velocity]
# Example: [0.01, 0.4, -0.05, 0.8]
```

### Output: The Q-Values

The network outputs **estimated scores** for each possible action.

```python
# CartPole Output (2 Q-values):
q_values = [Value_of_Pushing_Left, Value_of_Pushing_Right]
# Example: [12.5, 13.1]
```

### The Decision

Since `13.1 > 12.5`, we push **Right**.

```
The network does NOT output "Right".
It outputs "How good is Right?" (13.1) vs "How good is Left?" (12.5)
We pick the action with the higher value.
```

---

## Part 2: The "No Reference" Problem

This is where your supervised learning intuition fights you.

### In Supervised Learning (What You Know)

```
Input: Image of a Cat
Reference (Label): "Cat"  â† Human provided this!
Model Output: "Dog"
Loss: Wrong! The label says "Cat". Adjust weights.
```

The reference is a **human-provided label**.

### In Reinforcement Learning (The New Paradigm)

```
Input: Pole tilting at 0.1 radians
Action: Push Right
Result: Pole stays up for 1 more second
Reward: +1  â† This is our only feedback!

But what is the "correct" value for this state-action pair?
The Reference SHOULD be: "Immediate Reward + All Future Rewards"
Problem: We don't know the future rewards yet!
```

### The Solution: Bootstrapping

We **estimate** the future using our own brain!

```
Reference (Target) = Reward + Î³ Ã— max(Q(next_state))
                             â†‘
                     Our own estimate of the future
```

This is called **Bootstrapping** - we use a guess to train a guess. We're pulling ourselves up by our own shoelaces.

---

## Part 3: The Moving Target Problem

Here's where things get tricky with neural networks.

### In Tabular Q-Learning (Stable)

```python
Q[state, action] = Q[state, action] + Î± * (target - Q[state, action])
```

Updating cell `Q[5, Right]` does NOT affect cell `Q[3, Left]`. Each cell is independent.

### In Deep Q-Learning (Unstable!)

Updating one weight changes **ALL predictions simultaneously**.

```
Step 1: Network predicts Q = 0.5 for State A
        Target = 1 + 0.99 * Network(next_state) = 1.5
        Update weights to predict 1.5

Step 2: Network NOW predicts Q = 0.8 for State A (improved!)
        BUT the target ALSO changed: = 1 + 0.99 * Network(next_state) = 1.8
        (Because the network changed!)

Step 3: Q = 1.1, but Target = 2.0 (moved again!)

THE TARGET KEEPS RUNNING AWAY!
```

This is like a dog chasing its own tail. Or an archer shooting at a target that moves every time they adjust their aim.

---

## Part 4: The Two Brains Solution (Target Network)

### The Archer Analogy

**Scenario A: One Brain (Moving Target)**
1. You shoot an arrow. You miss slightly left.
2. You decide: "I need to aim more to the right."
3. **Problem:** As soon as you shift your aim, the target itself moves to the right (because the target is calculated by your own brain).
4. You shoot again. You miss again. You and the target spin in circles forever.

**Scenario B: Two Brains (Frozen Teacher)**
1. **Setup:** We put a cardboard cutout of the target on the wall. (This is the **Frozen Teacher/Target Network**).
2. **Steps 1-999:** You (The **Student/Policy Network**) practice shooting at this *stationary* cardboard cutout.
   - You shoot, measure the error, and update your aim.
   - You do this 1,000 times. You get really good at hitting that specific cutout.
   - *The Student IS training. Weights update every single shot.*
3. **Step 1000:** The Teacher wakes up and says: "Nice shooting. I've learned a lot watching you. Let me update my position."
4. **The Sync:** We move the cardboard cutout to the new, better location (copy Student weights to Teacher).
5. **Repeat:** Now spend the next 1,000 shots learning to hit the new location.

**Why This Works:** It converts a **Moving Target** problem into a series of **Stationary Target** problems.

---

## Part 5: The Calculator Analogy (Better Than Archery)

May be the archery analogy break down because it suggests a "fixed location" wrt to moving target which might not kick in for some people. Let's try another analogy.
In DQN, the frozen teacher is a **Frozen Calculator** - it gives different answers for different inputs, but those answers don't change during training.

### The Math Exam Analogy

Imagine you're a student taking a math exam.

**Scenario A: No Freeze (Unstable)**
1. **Input:** "2 + 2"
2. **Student:** "I think it's 3."
3. **Teacher:** "I think it's 5." (REMEMBER: Teacher is untrained/dumb)
4. **Student:** "Okay, I'll learn to output 5."
5. **BUT:** Since Teacher is the Student's clone, Teacher now thinks it's 6.
6. **Result:** Chaos. The answer keeps changing.

**Scenario B: Frozen Teacher (DQN)**
1. **Setup:** We clone the Teacher and tell him: *"Do not change your mind for 1,000 questions."*
2. **Question 1 (Input changes!):** "2 + 2"
   - Student: "I think 3."
   - Frozen Teacher: "My textbook says 4."
   - Update: Student learns to output 4 for "2 + 2".
3. **Question 2 (Input changes!):** "10 Ã— 10"
   - Student: "I think 50."
   - Frozen Teacher: "My textbook says 100."
   - Update: Student learns to output 100 for "10 Ã— 10".
4. **Question 3 (Input changes!):** "8 - 4"
   - Student: "I think 8."
   - Frozen Teacher: "My textbook says 4."
   - Update: Student learns to output 4 for "8 - 4".

NOTE: Teacher could be wrong too in Question 1, but it's consistent, that's what matters here. What we "freeze" in the target network is that the teacher's answers don't change during training. Even in the frozen state, the teacher is still a neural network and it gives different answers for different inputs. It's just that those answers don't change during training. The only thing that changes is the input to the teacher and the outputs will remain same for same inputs, this is what we "freeze" in the target network.

**Crucial Point:**
- The **Input** changes every single frame (new pole angle, new cart position).
- The **Frozen Teacher** gives *different answers for different inputs*.
- But those answers stay consistent during the training period.

We're **not** training to output "Left" always. We're training to **mimic the Teacher's consistent logic**.

---

## Part 6: The "Blind Leading Blind" Paradox

### Your Concern

> "If the Student learns from the Teacher, and the Teacher is a clone of the Student, isn't Loss = 0? How can blind lead blind?"

You're 100% correct! If `Loss = (Student - Teacher)` and they're identical clones, the Student learns nothing.

### The Missing Piece: Injection of Truth (The Reward!)

The Student does **NOT** try to mimic the Teacher directly.
The Student tries to mimic a **Formula** that mixes "Real Reality" with "Teacher's Guess."

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE FORMULA THAT BREAKS THE LOOP                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Target = Reward + Î³ Ã— Teacher(next_state)                          â”‚
â”‚           â†‘              â†‘                                          â”‚
â”‚           â”‚              â””â”€â”€ Teacher's (dumb) estimate              â”‚
â”‚           â”‚                                                         â”‚
â”‚           â””â”€â”€ REAL TRUTH from the environment!                      â”‚
â”‚                                                                     â”‚
â”‚  The Reward is the INJECTION OF TRUTH that breaks the loop!         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


$$Target = \underbrace{\text{Reward}}_{\text{REALITY (+1)}} + \underbrace{(\text{Discount} \times \text{Teacher's Guess})}_{\text{GUESS}}$$

The Student tries to match **THIS Target**, not just the Teacher.

---

### Step-by-Step: How Knowledge Is Created

**Setup:**
- Student: Random weights (dumb)
- Teacher: Clone of Student (equally dumb)
- Reality: Moving Right gives +1 Reward

**Step 0: The Blind Start**

1. **Input:** State A.

2. **Student Guess:** "I think value of Right is 0.5." (Random guess).

3. **Teacher Guess:** "I also think value of Right is 0.5." (Because it's a clone).

4. **Action:** Agent chooses Right.

5. **Environment:** BOOM! Reward +1. (This is the magic moment).

**Step 1: The Calculation (Where the Loop Breaks)**

Now we calculate the Loss.

- **Student's Prediction:** 0.5

- **Teacher's Prediction:** 0.5 (Still dumb).

### **The Target Formula (The Mix):**

$$Target = \text{Reward} + (\gamma \times \text{Teacher's Guess})$$

$$Target = \mathbf{1.0} + (0.99 \times 0.5)$$

$$Target = \mathbf{1.495}$$

**The Comparison:**

- Student said: 0.5

- The Target (Reality + Teacher) says: 1.495

**The Conflict:**

The Student realizes: "Wait a minute. My Teacher said 0.5. But because I actually found a gold nugget (+1), the real value is much higher (1.495). The Teacher was wrong!"

**The Update:**

The Student updates its weights to output closer to 1.495.

- **New Student Brain:** Now outputs 1.495 for State A.

- **Frozen Teacher Brain:** Still outputs 0.5 for State A.


### **Conclusion:** The Student has now **surpassed** the Teacher. It learned from the **Reward**, not just the Teacher.

---
###  How do we calculate the Loss?

> Doubt: "Do we calculate difference between output probabilities? Or rewards?"

In **DQN**, we calculate the difference between Q-Values (Estimated Total Scores).

1. **Student Output (Predicted Q):** A single number, e.g., 12.5.

2. **Target Value (Calculated Q):** The number from the formula above, e.g., 13.5.

3. **Loss Function (MSE):**

$$Loss = (\text{Target} - \text{Student})^2$$

$$Loss = (13.5 - 12.5)^2 = 1.0$$

4. **Backpropagation:** We tweak the Student's weights to make that 12.5 go up towards 13.5.

---
### Why Wait 1,000 Steps to Update Teacher?

Now that Student is smarter (outputs 1.495), why not update Teacher immediately?

Because of the **Moving Target** problem again!

If we update Teacher to 1.495 right now:
- Next Target = Reward + 0.99 Ã— 1.495 = even higher
- Then Target = even higher again...
- We're back to chasing our tail!

By keeping Teacher frozen for 1,000 steps:
```
Target = Reward_1 + (0.99 Ã— 0.5)  â† Stable baseline
Target = Reward_2 + (0.99 Ã— 0.5)  â† Same baseline
Target = Reward_3 + (0.99 Ã— 0.5)  â† Same baseline
...
```

The Student spends 1,000 steps collecting **Real Rewards** and adding them to the **Frozen Teacher's base value**. At Step 1,000, the Student says: *"Teacher, I've collected enough real data. Here are my improved weights."*

---

## Part 7: The Complete Picture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HOW KNOWLEDGE IS CREATED                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. Teacher starts dumb (random weights)                            â”‚
â”‚                                                                     â”‚
â”‚  2. Student starts dumb (clone of Teacher)                          â”‚
â”‚            Reality (+1) interferes.                                 â”‚
â”‚  3. Student plays game, receives REAL REWARDS (+1, -1)              â”‚
â”‚  4. Target = Reward + Î³ Ã— Teacher(next_state)                       â”‚
â”‚                  â†‘ REAL    â†‘ DUMB                                   â”‚
â”‚             This makes Target = "REAL + DUMB"                       â”‚
â”‚  5. Student learns to predict "REAL + DUMB" = 1 + (dumb guess)      â”‚
â”‚     (Student is now 'comparatively' smarter than Teacher!)          â”‚
â”‚  6. Teacher stays Dumb [Frozen] (provides stable baseline)          â”‚
â”‚                                                                     â”‚
â”‚ Loop: Student keeps adding Real Rewards to the Frozen Teacher's baseâ”‚
â”‚ Sync: Teacher finally accepts the new baseline.                     â”‚
â”‚  7. After N steps: Copy Student â†’ Teacher                           â”‚
â”‚     (Teacher adopts Student's improved estimates)                   â”‚
â”‚  8. Repeat with higher-quality baseline                             â”‚
â”‚                                                                     â”‚
â”‚  The staircase of improvement:                                      â”‚
â”‚                                                                     â”‚
â”‚     Step 3000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     â”‚
â”‚     Step 2000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                         â”‚
â”‚     Step 1000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                             â”‚
â”‚     Step 0:    â–ˆâ–ˆâ–ˆâ–ˆ                                                 â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 8: Tabular vs Deep Comparison

| Feature | Tabular Q-Learning | Deep Q-Learning |
|---------|-------------------|-----------------|
| **State Space** | Small, Fixed (16 squares) | Infinite (Continuous) |
| **Storage** | Table (Row 5, Col 2 = Value) | Neural Network weights |
| **Stability** | **Stable.** Updating Q[5,2] doesn't affect Q[3,1] | **Unstable.** Updating one weight changes ALL outputs |
| **The Fix** | None needed | **Target Network** (freeze the "future calculator") |
| **Update Rule** | `Q[s,a] += Î± * (target - Q[s,a])` | `loss = MSE(predicted, target); backprop` |

---

## Key Concepts Summary

### 1. Experience Replay

Store experiences in memory, sample randomly to break correlation.

```python
memory.push(state, action, reward, next_state, done)
batch = memory.sample(64)  # Random sample
```

**Why:** Sequential updates cause overfitting. Random sampling provides diverse training data.

### 2. Target Network

Two networks: Policy (Student) and Target (Frozen Teacher).

```python
policy_net = DQN(...)  # Updates every step
target_net = DQN(...)  # Frozen, synced every N episodes
```

**Why:** Provides stable learning target. Prevents "chasing your own tail."

### 3. Loss Function

MSE between predicted Q-value and target Q-value.

```python
current_q = policy_net(states).gather(1, actions)
target_q = rewards + GAMMA * target_net(next_states).max(1)[0] * (1 - dones)
loss = MSELoss(current_q, target_q)
```

**Why:** Gradient descent minimizes the difference between prediction and reality+future estimate.

---

## MY TAKEAWAYS

**ðŸ“ The "Student-Teacher" Stabilization Logic**

> "We utilize a dual-network architecture to stabilize learning, effectively creating a student-teacher dynamic where the 'Teacher' (Target Network) acts as a fixed reference point. Initially, the Teacher is untrained and provides a suboptimal baseline, but crucially, it remains frozen to allow the 'Student' (Policy Network) to practice against a stationary target rather than a chaotic, moving one. The Student does not rely solely on the Teacher's imperfect internal estimates; instead, it integrates External Rewards (Ground Truth) from the environment, which serve as the ultimate validation of whether an action was truly beneficial or detrimental. The Student continuously updates its internal memory based on this reality-checked feedback, surpassing the Teacher's initial knowledge. To prevent reference instability, we intentionally keep the Teacher uniformed for $N$ iterations; only after the Student has garnered sufficient verified knowledge do we synchronize the networks - updating the Teacher with the Studentâ€™s improved weights - thereby establishing a new, higher baseline for the next phase of learning."



The most critical part: **The improvement comes from the External Reward (Reality).** Without that injection of truth (+1 or -1), the Student and Teacher would just be agreeing on hallucinations. The Teacher provides stability; the Environment provides truth.

---

### ðŸ§  Concept: The Two-Brain Architecture (Student & Teacher)

**The Problem:** If a Neural Network generates its own training targets, it chases a moving target, causing instability (the "Dog chasing its tail" loop).

**The Solution:** Use two identical networks with a time delay.

1. **The Student (Policy Network):** The active agent. It interacts with the environment, takes risks, and updates its weights **every step**. It learns by combining **Real Rewards** (from the environment) with the **Teacher's Baseline** (future estimates).
2. **The Teacher (Target Network):** The stable reference. It is a **frozen clone** of the Student from the past. It provides a fixed baseline for the Student to improve upon, preventing the target from jittering.
3. **The Source of Truth:** Improvement does not come from the Teacher; it comes from the **External Rewards** (+1/-1). The Student uses these real rewards to realize the Teacher's baseline was an underestimate (or overestimate) and corrects itself.
4. **The Sync (Ratchet Effect):** Every N steps, the Student shares its new, verified knowledge with the Teacher. The Teacher updates to this new level, setting a higher bar for the next round of learning.

---

**In short:** The Teacher provides **Stability** (Consistency), while the Environment provides **Accuracy** (Truth). The Student bridges the gap.

---
---

## What Comes Next: Policy Gradients

DQN is powerful but has limitations:
- **DQN (Current):** Calculates a value (Q) for every action and picks the max. Good for simple choices (Left/Right).
- **Policy Gradient (Next):** Outputs probabilities directly ("80% thrust, 20% turn"). This is how LLMs work!

```
DQN:            State â†’ Network â†’ Q-values â†’ argmax â†’ Action
Policy Gradient: State â†’ Network â†’ Probabilities â†’ Sample â†’ Action

LLM:            Context â†’ Transformer â†’ Token Probs â†’ Sample â†’ Token
```

The Lunar Lander project introduces Policy Gradients, bringing us closer to the LLM architecture.

---

## Summary: The DQN Recipe

```
1. NEURAL NETWORK     â†’ Replace Q-table with function approximator
2. REPLAY BUFFER      â†’ Store experiences, sample randomly (break correlation)
3. TARGET NETWORK     â†’ Stable target (don't chase your tail)
4. REWARD INJECTION   â†’ Real rewards break the "blind leading blind" paradox
5. Îµ-GREEDY           â†’ Explore early, exploit later
6. BATCH TRAINING     â†’ Learn from multiple experiences at once
```

**The Core Insight:**
> The Target Network doesn't provide knowledge. It provides **STABILITY**.
> The **REWARD** provides knowledge - it's the injection of truth that breaks the loop.

----
NOTE: There's another explanation for the same topic, see **[DQN_EXPLAINED.md](../P2/DQN_EXPLAINED.md)**.