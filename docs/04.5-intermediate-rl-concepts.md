# Intermediate RL Concepts

> **Bridging Classical RL â†’ Deep RL**
> These concepts complete our classical RL foundation before moving to neural networks.

---

## 1. Custom RL Environments

Before, we played games others built (FrozenLake, CliffWalking). Now we learn to build our own simulations.

### The Three Organs of a Gym Environment

Every Gymnasium environment has three core methods:

```python
class ProductRecommendationEnv(gym.Env):
    
    def __init__(self, product_ctrs):
        """THE SETUP - Define the rules of the world"""
        super().__init__()
        self.product_ctrs = product_ctrs  # Secret truth (agent can't see this!)
        self.n_products = len(product_ctrs)
        self.action_space = spaces.Discrete(self.n_products)  # "You can pick Product 0, 1, 2, or 3"
        self.observation_space = spaces.Discrete(1)  # Placeholder for stateless tasks
    
    def reset(self, seed=None, options=None):
        """THE NEW GAME BUTTON - Start fresh"""
        super().reset(seed=seed)
        return 0, {}
    
    def step(self, action):
        """THE SIMULATOR - Core logic loop"""
        click = np.random.rand() < self.product_ctrs[action]
        reward = 10.0 if click else -10.0
        return 0, reward, False, False, {"clicked": click}
```

### ğŸ”§ CODEX Agent Connection

NOTE: CODEX is our IDE (Roopik coding agent, which we intend to use as a coding agent or as a judge. It's mentioned here to give analogy wrt what we intend to learn and build with this project.)

For our coding agent, the environment would be:

| Component | ProductRecommendation | CODEX (Roopik) |
|-----------|----------------------|----------------|
| **Action Space** | Pick Product 0-3 | Generate code / Pick tool |
| **Observation** | "User is on site" | Current code + error message |
| **Step Logic** | Simulate user click | Execute code in sandbox |
| **Reward** | +10 click, -10 no click | +10 compile, -10 error |

---

## 2. Multi-Armed Bandits (MAB)

The simplest form of RL - no states, just actions and rewards.

### The Scenario

```
You have 4 slot machines (arms). Each has a different hidden probability of winning.
You must figure out which one pays the most while also trying to make money.

Arms = Products to recommend
Pull = Showing a product to a user  
Reward = Did they click? (Yes/No)
Hidden Probability = True Click-Through Rate (CTR)
```

### Why MAB is Special

| Property | Grid World (FrozenLake) | Bandit (This) |
|----------|------------------------|---------------|
| **State affects future** | Yes (moving changes position) | No (each user is fresh) |
| **Memory needed** | Yes | No |
| **Complexity** | Higher | Simpler |

### The Metric: Click-Through Rate

```python
# Q-Value = "Batting Average" of each product
n_arms = 4
N = np.zeros(n_arms)  # Track times shown
C = np.zeros(n_arms)  # Track clicks  
Q = np.zeros(n_arms)  # Track observed CTR

# After each action:
N[action] += 1
C[action] += reward
Q[action] = C[action] / N[action]  # CTR = clicks / shows
```

---

## 3. Exploration Strategies for Bandits

### Strategy 1: Random Policy (Full Exploration)

```python
action = np.random.randint(n_arms)  # Just pick randomly
```

**Problem:** Never learns - stays at average luck forever.

### Strategy 2: Greedy Policy (Full Exploitation)

```python
minimum_tests = 5
if np.min(N) < minimum_tests:
    action = np.argmin(N)  # Test under-tested products first
else:
    action = np.argmax(Q)  # Always pick the current best
```

**Problem - The "Lucky Fool":**
```
Product A: Actually 10% CTR, but got lucky â†’ shows 80% in first 5 tries
Product B: Actually 50% CTR, but got unlucky â†’ shows 20% in first 5 tries

Greedy picks A forever. Never discovers B is better!
```

### Strategy 3: Epsilon-Greedy (Controlled Chaos)

```python
epsilon = 0.2
if np.random.rand() < epsilon:
    action = np.random.randint(n_arms)  # 20% explore
else:
    action = np.argmax(Q)  # 80% exploit
```

**The Balance:**
- Exploit (1âˆ’Îµ): "80% of the time, trust my data"
- Explore (Îµ): "20% of the time, try something random just in case"

### Strategy 4: Upper Confidence Bound (UCB) - Smart Explorer

```python
def calculate_ucb(Q, N, t, c=2):
    ucb_value = Q + c * np.sqrt(np.log(t) / (N + 1e-5))
    return ucb_value

action = np.argmax([calculate_ucb(Q[a], N[a], t) for a in range(n_arms)])
```

**The Formula:** `UCB = How Good + How Unknown`

```
UCB = Q(action) + c Ã— âˆš(ln(t) / N(action))
      â†‘              â†‘
      Exploitation   Exploration Bonus
      (Current avg)  (High if rarely tested)
```

**Example:**
```
Product A: High CTR (0.25), used 40 times â†’ "I know exactly who you are. Boring."
Product B: Lower CTR (0.15), used only 25 times â†’ "You might be amazing, I should test more!"

UCB picks B because uncertainty bonus is higher!
```

### ğŸ”§ CODEX Agent Connection: Tool Selection

```
Imagine CODEX has 3 tools to fix a bug:

Tool: Search StackOverflow â†’ Used 50 times, 40% success
Tool: Read Documentation â†’ Used 5 times, 30% success  
Tool: Analyze Git History â†’ Used 1 time, 0% success

Greedy: Always picks StackOverflow (40% is highest)
Epsilon: Usually picks SO, sometimes randomly tries Git
UCB: Picks Git! 

Why? "I can't conclude Git is useless from 1 try. 
      The uncertainty bonus is massive. Let me test it more."
```

---

## 4. Markov Decision Processes (MDP)

The formal framework that defines how agents interact with environments.

### The Markov Property: "Amnesia is a Superpower"

> "The outcome of the next step only depends on the current state, not the history."

```
Regular Coding: To fix a bug, you need logs, user steps, history
MDP: Agent only cares "I am at (2,3) with a passenger right NOW"
```

**Why This Matters:** Saves compute. If agent had to remember 10,000 steps to decide step 10,001, training would be impossible.

### MDP Components (Taxi Example)

| Component | Definition | Taxi Example |
|-----------|-----------|--------------|
| **States** | All possible situations | 500 states (position Ã— passenger Ã— destination) |
| **Actions** | What agent can do | Up, Down, Left, Right, Pickup, Dropoff |
| **Rewards** | Immediate feedback | +20 deliver, -1 step, -10 wrong pickup |
| **Transitions** | What happens after action | Deterministic: Up always moves up |
| **Discount (Î³)** | Future reward weight | 0.99 means future matters |

### The Discount Factor (Î³): The Marshmallow Test

```python
rewards = [-1, -1, -1, -1, -1, 20]  # 5 steps to deliver passenger

# Low gamma (0.3) - Impatient
low_return = (-1) + 0.3*(-1) + 0.3Â²*(-1) + 0.3Â³*(-1) + 0.3â´*(-1) + 0.3âµ*(20)
           = -1.37  # NEGATIVE! Agent thinks "Not worth the effort"

# High gamma (0.99) - Patient  
high_return = (-1) + 0.99*(-1) + 0.99Â²*(-1) + ... + 0.99âµ*(20)
            = +14.1  # POSITIVE! Agent sees the pot of gold at the end
```

### ğŸ”§ CODEX Agent Connection: Quick Fix vs Refactor

```
Scenario: CODEX sees messy code

Option A (Quick Fix): Add // @ts-ignore â†’ Fast, easy (Immediate -1)
Option B (Refactor): Rewrite the interface â†’ Hard, 5 steps (Future +20)

If Î³ is low (0.3): CODEX uses @ts-ignore (Lazy)
If Î³ is high (0.99): CODEX refactors (Smart, long-term thinking)
```

---

## 5. Transition Probabilities

The "Physics Engine" - what happens when you take an action.

### The Transition Dictionary (P)

```python
env = gym.make('Taxi-v3')
P = env.unwrapped.P  # The complete "Rule Book"

# P[state][action] = [(probability, next_state, reward, done)]

P[0][0]  # State 0, Action 0 (Down)
# â†’ [(1.0, 100, -1, False)]  
# "100% chance â†’ move to state 100, get -1 reward, not done"
```

### Deterministic vs Stochastic

| Type | Probability | Example |
|------|-------------|---------|
| **Deterministic** | Always 1.0 | Taxi: Up always moves up |
| **Stochastic** | Variable | FrozenLake: Ice makes you slip randomly |

### ğŸ”§ CODEX Agent Connection

```
Deterministic Tool (Safe):
- Action: Prettier Format
- Result: 100% formats code, 0% breaks logic
- Strategy: Use fearlessly

Stochastic Tool (Risky):
- Action: npm install --legacy-peer-deps
- Result: 60% works, 40% breaks node_modules
- Strategy: Only use if reward is massive (+100 fix critical bug)
```

---

## 6. Monte Carlo Methods

Learning by completing full episodes, then looking back.

### Q-Learning vs Monte Carlo

| Q-Learning | Monte Carlo |
|------------|-------------|
| "Impulsive Learner" | "Patient Analyst" |
| Update after EVERY step | Update after episode ENDS |
| Step â†’ Reward â†’ Update NOW | Play â†’ Record â†’ Analyze at end |

```
Q-Learning (Linter):
  Agent writes: const x = ;
  Feedback: "Syntax Error!"
  Update: Fix immediately

Monte Carlo (Deployment):
  Agent: Write â†’ Commit â†’ Push â†’ CI/CD â†’ Staging â†’ Crash
  Feedback: "App crashed on startup"
  Update: Look at entire sequence to find the cause
```

### First-Visit vs Every-Visit MC

**Scenario:** Taxi gets lost, drives in a circle: `A â†’ B â†’ A â†’ Goal`

**Rewards:** -1, -1, -1, +10

#### First-Visit MC (Grade once per episode)

```
Find first time at State A (t=0)
Calculate total from there to end:
  G = (-1) + (-1) + (-1) + (+10) = 7

"One episode, one vote for State A"
```

#### Every-Visit MC (Grade every occurrence)

```
Visit 1 (t=0): Future = loop + goal â†’ G = 7
Visit 2 (t=2): Future = goal only â†’ G = (-1) + 10 = 9

Average for A = (7 + 9) / 2 = 8
```

**Why Every-Visit is often better for agents:**
```
If agent tries "npm install" twice and fails BOTH times:
- Every-Visit: Penalizes TWICE â†’ Strong signal "stop doing this"
- First-Visit: Ignores second failure â†’ Agent might repeat loops
```

---

## 7. Hyperparameter Tuning

The "knobs" that control learning speed and stability.

### Learning Rate (Î±)

```python
# Instead of averaging all returns (slow, memory-heavy):
q_table[s, a] = np.mean(returns[(s, a)])

# Use incremental update (fast, efficient):
q_table[s, a] += alpha * (G - q_table[s, a])
```

| Î± Value | Behavior |
|---------|----------|
| High (0.9) | Learns fast, but unstable (swings wildly) |
| Low (0.01) | Stable, but learns very slowly |

### Epsilon Decay

```python
epsilon = 0.9       # Start curious
min_epsilon = 0.1   # Stay slightly curious  
decay_rate = 0.9995

for episode in range(n_episodes):
    # ... training loop ...
    epsilon = max(min_epsilon, epsilon * decay_rate)
```

**Strategy:** Start high Îµ (explore when clueless) â†’ End low Îµ (exploit when expert)

### The Three Main Knobs

| Hyperparameter | Controls | If too high | If too low |
|---------------|----------|-------------|------------|
| **Learning Rate (Î±)** | Update speed | Unstable, oscillates | Learns too slowly |
| **Discount (Î³)** | Future vs now | May delay reward forever | Myopic, ignores future |
| **Epsilon (Îµ)** | Explore vs exploit | Never settles | Stuck on suboptimal |

---

## 8. Model-Based vs Model-Free RL

The fundamental divide in RL approaches.

### What is a "Model"?

Not the neural network! A **Model** is a simulation of the world:
- **Transition Function:** "If I drop this glass, it falls" (Physics)
- **Reward Function:** "If glass breaks, I get yelled at" (Rules)

### The Two Schools

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MODEL-FREE (The Skater)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Algorithms: Q-Learning, SARSA, Monte Carlo, PPO                    â”‚
â”‚  Method: Trial and error. Don't know the rules, just try things.   â”‚
â”‚  Pros: Can learn anything without understanding how it works        â”‚
â”‚  Cons: Must crash a lot to learn                                    â”‚
â”‚  Example: Riding a bicycle - can't calculate physics, just balance  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MODEL-BASED (The Architect)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Algorithms: Dynamic Programming, AlphaZero                         â”‚
â”‚  Method: Planning. Have the rule book, calculate perfect path.     â”‚
â”‚  Pros: Zero crashes, solve perfectly first try                      â”‚
â”‚  Cons: Need the complete rule book (rare in real world)             â”‚
â”‚  Example: Chess - rules never change, board is small, calculate all â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”§ Why CODEX Must Be Model-Free

```
Model-Free Agent (What we're building):
  Tries: import { Button } from 'react-button'
  Runs build â†’ Error: "Module not found"
  Updates: "Okay, let me try import { Button } from './components'"
  Learns by crashing.

Model-Based Agent (The Dream):
  Reads package.json and file structure first
  Constructs internal "Model" of the codebase
  Thinks: "Button is at ./ui/Button, I'll write it correctly first time"
  Plans before typing.

Why Model-Free wins for coding:
  - Every program is unique (new language, new requirements)
  - Can't mathematically prove how npm works
  - Too many states to model (infinite code combinations)
```

---

## 9. Dynamic Programming (Value Iteration)

When you have the complete model, you can "solve" instead of "learn."

### The Concept: Propagating Truth

Value Iteration doesn't run around the maze. It works like a ripple:

```
Step 1: Assume every square = 0
Step 2: Square next to Goal (+20) realizes: "I'm valuable!"
Step 3: Square next to THAT realizes: "My neighbor is rich, I'm valuable too!"
Step 4: Keep spreading until values stop changing (convergence)
```

### The Bellman Equation

```python
V = np.zeros(n_states)
Q = np.zeros((n_states, n_actions))

for iteration in range(max_iterations):
    delta = 0
    old_V = V.copy()
    
    for state in range(n_states):
        for action in range(n_actions):
            action_value = 0
            for prob, next_state, reward, done in P[state][action]:
                if done:
                    action_value += prob * reward
                else:
                    action_value += prob * (reward + gamma * V[next_state])
            Q[state, action] = action_value
        
        V[state] = np.max(Q[state])
        delta = max(delta, abs(old_V[state] - V[state]))
    
    if delta < theta:  # Converged!
        break
```

**Formula:** `Value = Î£(Probability Ã— (Immediate Reward + Future Value))`

---

## 10. When to Use What

| Scenario | Approach | Why |
|----------|----------|-----|
| **FrozenLake** | Model-Based | Rules known, small state space |
| **Warehouse Robot** | Model-Free | Layout changes daily, unpredictable |
| **Self-Driving Car** | Hybrid | Model for GPS, Model-Free for pedestrians |
| **Coding Agent (CODEX)** | Model-Free (PPO) | Infinite states, unpredictable code |

---

## Summary: The Bridge to Deep RL

```
What We've Learned:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Custom Environments â†’ How to wrap any problem as RL
2. Multi-Armed Bandits â†’ Simplest RL, exploration strategies
3. MDPs â†’ The formal framework (states, actions, transitions)
4. Monte Carlo â†’ Learning from complete episodes
5. Model-Based vs Model-Free â†’ Calculate vs Experience
6. Dynamic Programming â†’ Solving when you have the model

What's Next (Deep RL):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
These "Q-Tables" work for 500 states (Taxi).
But for images (millions of pixels) or code (infinite combinations)?
We replace tables with Neural Networks!

Q-Table:    Q[state][action] = lookup
DQN:        Q(state) = NeuralNet(state) â†’ values
Policy:     Ï€(action|state) = NeuralNet(state) â†’ probabilities
LLM:        Ï€(token|context) = Transformer(context) â†’ probabilities

It's the same concepts, just with bigger neural networks!
```

---

---
 We have officially graduated from "Grid World." You now understand the *mechanics* of how machines learn from rewards.. if not deeply, kind of at high level at least. Right? (-_-)

However, there is one massive problem we need to fix before we can build **CODEX (Roopik)**.

### The Problem: The "Table" is Dead

In the Taxi game (simulated gym environment examples), we had 500 states. We made a table with 500 rows.
In your Coding Agent, the "State" is the current code in the editor.

* **Question:** How many possible Python programs exist?
* **Answer:** Infinite.
* **Consequence:** You cannot build a Q-Table with infinite rows. Your RAM would explode.

### The Solution: Project 2 (Deep RL)

We are going to stick to your original plan. We need to replace that Excel Sheet (Q-Table) with a **Neural Network**.

Instead of *looking up* the value of a state, we will train a Brain to *predict* the value.

---

### ğŸ’¡ The Roadmap: From Toy Taxi to CODEX (Coding Agent for Roopik IDE)

We are now entering **Phase 2: Deep Reinforcement Learning.** Here is the  plan to get us to our LLM Agent:

#### **Step 1: The "Hello World" of Deep RL (DQN)**

* **The Project:** **CartPole** (Balancing a pole on a cart).
* **The Tech:** Deep Q-Networks (DQN).
* **The Lesson:** How to wire a **PyTorch/TensorFlow Neural Network** into a Gymnasium environment. You will learn about "Replay Buffers" (Memory) and replacing the Q-Table equation with a Loss Function.
* **Why for Roopik?** This teaches you how to handle continuous states (like floating point numbers) which is the first step away from "grids."

#### **Step 2: The "Actor" (Policy Gradients / REINFORCE)**

* **The Project:** **Lunar Lander** (Landing a spaceship).
* **The Tech:** Policy Gradients.
* **The Lesson:** Instead of calculating values (Q), the agent just learns a "gut feeling" (Policy).
* **Why for Roopik?** LLMs don't calculate Q-values for every word. They output probabilities directly. This is the architecture used for text.

#### **Step 3: The "ChatGPT" Algorithm (PPO)**

* **The Project:** **Custom Text Environment**.
* **The Tech:** Proximal Policy Optimization (PPO).
* **The Lesson:** This is the exact algorithm used to train ChatGPT and Claude. We will fine-tune a tiny language model to output positive sentiments or specific formats.
* **Why for Roopik?** This **IS** the algorithm you will use for CODEX.

---

**Ready for Deep RL?** 

Now we take everything learned here and scale it up:
- **P2:** Replace Q-Table with Neural Network (DQN on CartPole)
- **P3:** Policy Gradient + Actor-Critic (LunarLander)
- **P4:** PPO - The heart of RLHF
- **P5/P6:** Apply to LLMs and CODEX (IDE)!
