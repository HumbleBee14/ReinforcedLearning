# P3: Policy Gradient & Actor-Critic

> **Goal:** Move from value-based (DQN) to policy-based methods - the foundation for LLM RL.

## What We'll Learn

- Policy Gradient theorem (REINFORCE)
- Why we need policy methods for LLMs
- Actor-Critic architecture
- Advantage function (A = Q - V)
- Baseline reduction for variance

## Key Bridge Concept

```
DQN:           Learn Q(s,a), derive policy from argmax
     â†“
Policy Gradient: Learn Ï€(a|s) DIRECTLY

LLMs ARE policies! Ï€(next_token | context) = the LLM itself!
```

## Prerequisites

- âœ… P0, P1: Tabular RL
- âœ… P2: DQN (neural network as function approximator)
- âœ… Docs 00-05

## Environment

**LunarLander-v2** or **CartPole-v1**
- Continuous or discrete actions
- More complex than CartPole

## Files (To Be Created)

```
P3/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ reinforce.py           # Basic REINFORCE algorithm
â”œâ”€â”€ actor_critic.py        # A2C implementation
â”œâ”€â”€ policy_network.py      # Policy network architecture
â””â”€â”€ train.py               # Training script
```

## Status: ðŸ”œ Coming After P2
